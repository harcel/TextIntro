{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aca6937",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/1869px-Python-logo-notext.svg.png\" align='right' width=200>\n",
    "\n",
    "# Text analysis in Python\n",
    "## An introduction from scratch\n",
    "\n",
    "\n",
    "Marcel Haas, Hielke Muizelaar (LUMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc63eb1-8e01-4c6a-88b3-ff0c98b0cdb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP related\n",
    "import string\n",
    "import regex as re\n",
    "import spacy\n",
    "\n",
    "# Machine learning\n",
    "import sklearn\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b9c16",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Print which versions are used\n",
    "print(\"This notebook uses the following packages (and versions):\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"python\", sys.version[:6])\n",
    "print('\\n'.join(f'{m.__name__} {m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fa9fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The <i>very</i> basics\n",
    "## String operations in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2650847",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"This workshop is about language, and Python. Let's Go!\"\n",
    "sentences = my_text.split('. ')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10e3e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Removing punctuation with regular expressions\n",
    "def remove_punctuation(text):\n",
    "    pattern = \"[\" + string.punctuation + \"]+\"\n",
    "    result = re.sub(pattern,\" \",text)\n",
    "    return result\n",
    "\n",
    "print(remove_punctuation(\"text!!!text??\"))\n",
    "\n",
    "\n",
    "for s in sentences:\n",
    "    print(remove_punctuation(s.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40c4c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SpaCy language models\n",
    "\n",
    "Much of what's here is adapted from the [spaCy documentation](https://spacy.io/).\n",
    "\n",
    "There are many complications. In most applications, you will be after something like *the meaning*, *the context* or *the intent* of text. These can be hard to extract, and we will look at the quantification of text in steps.\n",
    "\n",
    "From spaCy you can import [pre-trained language models](https://spacy.io/usage/models) in a number of languages, that enable you to digest the \"documents\" (this can be just that example sentence, or a whole collection of books). The examples below show what you can do with such \"NLP models\".\n",
    "\n",
    "## This is the first step from <i>text</i> to <i>quantitative data</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4d608",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part-of-Speech Tagging\n",
    "POS tagging can be helpful for understanding the build-up of the text you're dealing with. See below for an example.\n",
    "\n",
    "Let's start with a simple example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759efdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is an example sentence by Marcel with an obvouis spelling mistake.\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # load a language model\n",
    "doc = nlp(sentence)                 # Process the sentence with it into a \"document\"\n",
    "for token in doc:                   # Show some added attributes\n",
    "    print(f\"{token.text:14s} {token.pos_:6s} {token.dep_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f74b1",
   "metadata": {},
   "source": [
    "Note: If the language model doesn't load:\n",
    ">python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e9fdf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you need to know what any of those abbreviations mean, you can invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801e8fb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spacy.explain(\"ADJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aad39e",
   "metadata": {},
   "source": [
    "The interplay of words within a sentence is also known to the `doc` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382df779",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d89fec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named entity recognition\n",
    "\n",
    "Also a part of the language model, entities can be recognized.\n",
    "SpaCy understands that my name is a \"named entity\" and it can try to figure out what kind of an entity I am:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents: print(f\"{ent} is a {ent.label_} and appears in the sentence at position {ent.start_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ec524",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stop words\n",
    "\n",
    "What a stop word is <i><b>should</b></i> depend on your use case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e76fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(f\"I know {len(stopwords)} stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859f3b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text normalization: \n",
    "## Stemming and lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6819b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_the_news = [\"Belarus has been accused of taking revenge for EU sanctions by offering migrants \" \\\n",
    "\"tourist visas, and helping them across its border. The BBC has tracked one group trying to reach Germany.\", \n",
    "                \"Biden and senators on verge of striking immigration deal aimed at clamping down on illegal border crossings\",\n",
    "                \"Tea with salt? American scientist's \\\"outrageous proposal\\\" leaves U.S.-U.K. relations in \\\"hot water,\\\" embassy says\",\n",
    "                \"Apple's Stolen Device Protection feature is now live. Here's how it can help protect your iPhone.\",\n",
    "                \"Doomsday clock time for 2024 remains at 90 seconds to midnight. Here's what that means.\",\n",
    "                \"Microsoft Teams services are down, as thousands of users report issues\",\n",
    "                \"Funeral homes warned after FTC's first undercover phone sweep reveals misleading pricing\",\n",
    "                \"How to watch today's Kansas City Chiefs vs. Baltimore Ravens game: AFC Championship Game livestream options\"\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(from_the_news[0])\n",
    "\n",
    "lemma_word1 = [] \n",
    "for token in doc:\n",
    "    lemma_word1.append(token.lemma_)\n",
    "' '.join(lemma_word1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d24056",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating features for Machine Learning\n",
    "\n",
    "![features](Figures/features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e40247",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![bag](Figures/bag.png)\n",
    "![bag](Figures/bagwords.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170a35e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Python wouldn't be python if...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b12e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2,\n",
    "                             max_df=.9,\n",
    "                             ngram_range=(1,2))\n",
    "\n",
    "bow = vectorizer.fit_transform(from_the_news)\n",
    "\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b40b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Text Classification example\n",
    "\n",
    "- 20 Newsgroups has short texts about specific topics. \n",
    "\n",
    "- We load 4 of them, and then try to predict the topic based on the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90baea19",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22311448",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# We will load only 4 of the categories\n",
    "cats = ['sci.space', 'sci.med', 'rec.autos', 'alt.atheism']\n",
    "data = fetch_20newsgroups(categories=cats, \n",
    "                          remove=('headers', 'footers'))\n",
    "\n",
    "print(data.target.shape)\n",
    "print(len(data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b8b9d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get a random one\n",
    "random_index = np.random.randint(0, high=len(data.data))\n",
    "print(data.target_names[data.target[random_index]])\n",
    "print()\n",
    "print(data.data[random_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1e37d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's do some pre-processing\n",
    "\n",
    "This is 1337 for \"transform the data into usable form\".\n",
    "(trust me on this one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a1942",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "    return text.str.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    pattern = \"[\" + string.punctuation + \"]+\"\n",
    "    text = text.str.replace(pattern, \" \", regex=True)\n",
    "    text = text.str.replace(\"\\n\", \" \", regex=False)\n",
    "    return text\n",
    "\n",
    "def lemmatize_stopwords(s):\n",
    "    # I combine lemmatization and stopword removal to\n",
    "    # have them both use nlp()\n",
    "    # This is the slow function! Can you do something about it?\n",
    "    doc = nlp(s)\n",
    "    lemma = [] \n",
    "    for token in doc:\n",
    "        if token.text not in stopwords:\n",
    "            lemma.append(token.lemma_)\n",
    "    return ' '.join(lemma)\n",
    "\n",
    "def lsw(text):\n",
    "    return text.apply(lemmatize_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7654113",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':data.data, 'target':data.target})\n",
    "\n",
    "processed = (df.text.pipe(to_lower)\n",
    "                    .pipe(remove_punctuation)\n",
    "                    .pipe(lsw)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d91bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vectorize the cleaned data\n",
    "\n",
    "Vectorization: transforming the unstructured data into rows of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=4,\n",
    "                             max_df=.5,\n",
    "                             ngram_range=(1,2))\n",
    "bow = vectorizer.fit_transform(processed)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf76ed4",
   "metadata": {},
   "source": [
    "## We have a feature matrix for a prediction model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05924bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive modeling\n",
    "\n",
    "\n",
    "Let's do prediction. This will be a lot of code again, but don't you worry...\n",
    "\n",
    "1. Import the things we need\n",
    "2. Split into a train and test set, so we can honestly assess the predictive power\n",
    "3. Train a Naive Bayes model on the training data\n",
    "4. Inspect the confusion matrix to see how we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71db838",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, data.target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551a58e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Instantiate and train model\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Predict test data and assess!\n",
    "y_pred = NB_model.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16df28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# I use a logarithmic color scheme, so the few mismatches are visible!\n",
    "plt.imshow(np.log(cm+1), interpolation='nearest', cmap=plt.cm.Blues)\n",
    "tick_marks = np.arange(len(data.target_names))\n",
    "plt.xticks(tick_marks, data.target_names, rotation=45, ha=\"right\")\n",
    "plt.yticks(tick_marks, data.target_names)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "cb = plt.colorbar()\n",
    "cb.set_ticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fd92c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bags-of-Words are unaware of the context and meaning of words\n",
    "\n",
    "Back to SpaCy, because they don't have to be!\n",
    "\n",
    "Word vectors embed a word, based on their \"surroundings\" into a high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0242d5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word vectors from very simple language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mango = nlp('mango')\n",
    "mango.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mango = nlp('mango').vector\n",
    "strawberry = nlp('strawberry').vector\n",
    "brick = nlp('brick').vector\n",
    "\n",
    "print(((mango - strawberry)**2).sum())\n",
    "print(((mango - brick)**2).sum())\n",
    "print(((strawberry - brick)**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc3aea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Similarity is also baked in, but only for \"decent\" language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b27307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not download the larger language model like below,\n",
    "# spaCy will complain about not having word vectors for the similarity\n",
    "# measures. Download the larger model to your computer using \n",
    "# $ python -m spacy download en_core_web_lg\n",
    "# Note that this takes about 800 MB of disk space\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokens = nlp('mango strawberry brick')\n",
    "for i, token1 in enumerate(tokens):\n",
    "    # similarities are symmetric!\n",
    "    for token2 in tokens[i+1:]:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e5de7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On to Transformer-based language models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e77bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo.png\" align='right' width=200>\n",
    "\n",
    "## Hugging Face\n",
    "https://www.huggingface.co\n",
    "### A platform for machine learning applications, with a large focus on (large) language models!\n",
    "\n",
    "Most known for its \"transformers\" library, which provides thousands of pretrained models to perform tasks on several modalities, including text. \n",
    "\n",
    "The library is named after the transformer neural network architecture â†“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9954cc0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![transformers](Figures/transformers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03edaf21",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### If this looks complex to you, you are absolutely right!\n",
    "\n",
    "\n",
    "First described in a 2017 paper by Google, the transformer model is a sequence-to-sequence model that has seen widespread use in a plethora of machine learning tasks. The architecture was proposed to solve the problem of neural machine translation, which applies to any task that transforms an input sequence to an output sequence.\n",
    "\n",
    "A large part of what sets transformers apart from other (priorly used) neural network architectures is the inclusion of attention mechanisms. In contrast to assigning the most recent words the highest importance, attention works similarly to our brains, in that it assigns larger weights to more influential words in a sentence. \n",
    "\n",
    "Classic transformer models contain multiple encoders and decoders. Encoders (shown left in the image above) take in an input sequence, text in our case, and transform these sequences to context vectors. For text, this generally means transforming the text to representative numerical values. These values are fed through a series of layers that reduce their dimensionality while preserving relevant information about how they relate to one another within the sentence structure. This is called \"encoding\" the input sequence. The decoder is then responsible for transforming this encoded representation back to its original form, in this case also taking the attention weights for each sequence part in account.\n",
    "\n",
    "There is much more to transformer models than we can mention here. If you are interested in a deeper dive into the weeds of the workings of the model there are some great resources available online:\n",
    "\n",
    "https://arxiv.org/abs/1706.03762 \n",
    "\n",
    "https://blog.research.google/2017/08/transformer-novel-neural-network.html\n",
    "\n",
    "https://www.turing.com/kb/brief-introduction-to-transformers-and-their-power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8353bac8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A popular natural language processing transformer model is BERT (Bidirectional Encoder Representations from Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3791159",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![bert](Figures/bert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b3e04",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "BERT only includes the encoder part of the transformer model. This means that, on its own, it is only able to transform text to numerical values, NOT generate text. BERT was the first model to evaluate words both forwards and backwards, resulting in a deeper contextual understanding of the subject matter. During training, BERT masks a set percentage of the words in a sequences and attempts to predict the masked word by evaluating the words around it, rather than just evaluating the words either before or after, which was standard practice at the time. \n",
    "\n",
    "BERT can be fine-tuned for different types of tasks, such as text classification. This is done by adding a classification layer on top. This approach has seen widespread success and has contributed to BERT becoming the state-of-the-art model in the text classification domain. \n",
    "\n",
    "### Let's explore how we can use BERT ourselves using the aforementioned Hugging Face resources!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c8e03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e2c44",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from huggingface_hub import notebook_login\n",
    "import evaluate\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be684ea5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We recommend to create a Hugging Face account, with which you can obtain access tokens and host your own models on the site!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f7b91d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Enter token\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e33b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load the dataset\n",
    "\n",
    "For this example we use the same 20 newsgroups dataset from earlier. We use a Hugging Face 'Dataset' object, as they work well within the Hugging Face ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410159fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "id2label = {idx: label for idx, label in enumerate(data[\"target_names\"])}\n",
    "d = {\"text\": data[\"data\"], \"label\": data[\"target\"]}\n",
    "dset = Dataset.from_dict(d)\n",
    "dset = dset.map(lambda x: {\"label_text\": id2label[x[\"label\"]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c09bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialize tokenizer\n",
    "\n",
    "Tokenization is an essential part of natural language processing. It describes the process of breaking text into smaller parts, making machine analysis feasible. For BERT, the WordPiece algorithm is used, which creates a vocabulary of subwords based on frequently occurring symbol pairs in the training set.\n",
    "\n",
    "In this example we use DistilBERT, a distilled version of BERT that has 40% less parameters than regular BERT, resulting in significantly faster load and training times, while preserving 95% of its performance. Here, we load its tokenizer from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7df363",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b711991",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenize the dataset\n",
    "\n",
    "We now use the tokenizer on the dataset to obtain a tokenized version. We firstly create a function which does this and trims the text to the maximum input size of DistilBERT (truncation), as it cannot handle more than 512 tokens at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20df427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_groups = dset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720e5f81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Split dataset to training and test set\n",
    "\n",
    "\n",
    "In machine learning it is standard practice to utilise a training and a test set. The training set is merely used to train the model, while the model does not interact with the test set until after it is fully trained. By evaluating the model on an unseen set we can ensure it is not merely replicating what it has learned, but is rather able to apply learned information to new instances.\n",
    "\n",
    "In this example we use 70% of our total set for training and the remaining 30% for testing/evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dset_split = tokenized_groups.train_test_split(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc2c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31668e14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialize data collator\n",
    "\n",
    "Data collators are used to create several batches of the input data, allowing for batched training, which speeds up training time and affects generalization of the model. Here, we use a padded data collator which dynamically pads input sequences with differing lengths to obtain sequences with the same length, which is required for batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9217ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305476d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load model\n",
    "\n",
    "\n",
    "As we are fine-tuning an existing model for classification, we have to load said model from Hugging Face. We specify that we use the model for a text classification task via the use of \"AutoModelForSequenceClassification\". We set the number of labels to 20 as we have 20 different news groups in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6553a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90f404",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Initialize performance metrics\n",
    "\n",
    "Naturally, we want to record the performance of our classification model. For this instance, we use \"accuracy\", which comes down to the percentage of total instances our model classified correctly. We make sure the predictions are matched with the actual labels (classes) linked to each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c63463",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fine-tune/train the model\n",
    "\n",
    "Finally, we move towards training (in this case fine-tuning) DistilBERT on our data. We specify an output directory for the finished model, which can be either a local path or a name for a Hugging Face repository. The other training arguments are chosen here arbitrarily for illustrative purposes. It is recommended to test several setups of arguments in order to explore what works on the data-at-hand. For our trainer-object we specify which dataset needs to be used for training and which for evaluation, and we pass our data collator and performance metric calculator from before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ba365",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"Hielke/20newsgroups_healthRI\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=groups_dset_split[\"train\"],\n",
    "    eval_dataset=groups_dset_split[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f52cbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classify new text using the model!\n",
    "\n",
    "With the model now trained, we can use it for classification purposes on whatever we deem worthy for it! For this we create a pipeline object, which handles tokenization and other processing steps of the given input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dabef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('text-classification', model='C:/users/muize/Hielke/20newsgroups_healthRI')\n",
    "\n",
    "classifier(\"Is man merely a mistake of God's? Or God merely a mistake of man? -Friedrich Nietzsche\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

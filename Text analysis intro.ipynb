{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aca6937",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c3/Python-logo-notext.svg/1869px-Python-logo-notext.svg.png\" align='right' width=200>\n",
    "\n",
    "# Text analysis in Python\n",
    "## An introduction from scratch\n",
    "\n",
    "\n",
    "Marcel Haas, Hielke Muizalaar (LUMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc63eb1-8e01-4c6a-88b3-ff0c98b0cdb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLP related\n",
    "import string\n",
    "import regex as re\n",
    "import spacy\n",
    "\n",
    "# Machine learning\n",
    "import sklearn\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "365b9c16",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook uses the following packages (and versions):\n",
      "---------------------------------------------------------\n",
      "python 3.11.7\n",
      "spacy 3.7.2\n",
      "numpy 1.26.3\n",
      "pandas 2.2.0\n",
      "regex 2.5.140\n",
      "sklearn 1.4.0\n"
     ]
    }
   ],
   "source": [
    "# Print which versions are used\n",
    "print(\"This notebook uses the following packages (and versions):\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"python\", sys.version[:6])\n",
    "print('\\n'.join(f'{m.__name__} {m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fa9fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The <i>very</i> basics\n",
    "## String operations in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2650847",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"This workshop is about language, and Python. Let's Go!\"\n",
    "sentences = my_text.split('. ')\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f10e3e4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Removing punctuation with regular expressions\n",
    "def remove_punctuation(text):\n",
    "    pattern = \"[\" + string.punctuation + \"]+\"\n",
    "    result = re.sub(pattern,\" \",text)\n",
    "    return result\n",
    "\n",
    "print(remove_punctuation(\"text!!!text??\"))\n",
    "\n",
    "\n",
    "for s in sentences:\n",
    "    print(remove_punctuation(s.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40c4c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SpaCy language models\n",
    "\n",
    "Much of what's here is adapted from the [spaCy documentation](https://spacy.io/).\n",
    "\n",
    "There are many complications. In most applications, you will be after something like *the meaning*, *the context* or *the intent* of text. These can be hard to extract, and we will look at the quantification of text in steps.\n",
    "\n",
    "From spaCy you can import [pre-trained language models](https://spacy.io/usage/models) in a number of languages, that enable you to digest the \"documents\" (this can be just that example sentence, or a whole collection of books). The examples below show what you can do with such \"NLP models\".\n",
    "\n",
    "## This is the first step from <i>text</i> to <i>quantitative data</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4d608",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part-of-Speech Tagging\n",
    "POS tagging can be helpful for understanding the build-up of the text you're dealing with. See below for an example.\n",
    "\n",
    "Let's start with a simple example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759efdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is an example sentence by Marcel with an obvouis spelling mistake.\"\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # load a language model\n",
    "doc = nlp(sentence)                 # Process the sentence with it into a \"document\"\n",
    "for token in doc:                   # Show some added attributes\n",
    "    print(f\"{token.text:14s} {token.pos_:6s} {token.dep_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f74b1",
   "metadata": {},
   "source": [
    "Note: If the language model doesn't load:\n",
    ">python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e9fdf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you need to know what any of those abbreviations mean, you can invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801e8fb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spacy.explain(\"ADJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aad39e",
   "metadata": {},
   "source": [
    "The interplay of words within a sentence is also known to the `doc` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382df779",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d89fec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named entity recognition\n",
    "\n",
    "Also a part of the language model, entities can be recognized.\n",
    "SpaCy understands that my name is a \"named entity\" and it can try to figure out what kind of an entity I am:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents: print(f\"{ent} is a {ent.label_} and appears in the sentence at position {ent.start_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1ec524",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stop words\n",
    "\n",
    "What a stop word is <i><b>should</b></i> depend on your use case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e76fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(f\"I know {len(stopwords)} stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5859f3b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Text normalization: \n",
    "## Stemming and lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6819b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from_the_news = \"Belarus has been accused of taking revenge for EU sanctions by offering migrants \" \n",
    "\"tourist visas, and helping them across its border. The BBC has tracked one group trying to reach Germany.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638cca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(from_the_news)\n",
    "\n",
    "lemma_word1 = [] \n",
    "for token in doc:\n",
    "    lemma_word1.append(token.lemma_)\n",
    "' '.join(lemma_word1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d24056",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating features for Machine Learning\n",
    "\n",
    "![features](Figures/features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e40247",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![bag](Figures/bag.png)\n",
    "![bag](Figures/bagwords.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9170a35e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Python wouldn't be python if...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b12e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=4,\n",
    "                             max_df=.5,\n",
    "                             ngram_range=[1,2])\n",
    "\n",
    "bow = vectorizer.fit_transform(from_the_news)\n",
    "\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b40b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Text Classification example\n",
    "\n",
    "- 20 Newsgroups has short texts about specific topics. \n",
    "\n",
    "- We load 4 of them, and then try to predict the topic based on the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90baea19",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22311448",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# We will load only 4 of the categories\n",
    "cats = ['sci.space', 'sci.med', 'rec.autos', 'alt.atheism']\n",
    "data = fetch_20newsgroups(categories=cats, \n",
    "                          remove=('headers', 'footers'))\n",
    "\n",
    "print(data.target.shape)\n",
    "print(len(data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b8b9d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Get a random one\n",
    "random_index = np.random.randint(0, high=len(data.data))\n",
    "print(data.target_names[data.target[random_index]])\n",
    "print()\n",
    "print(data.data[random_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1e37d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's do some pre-processing\n",
    "\n",
    "(trust me on this one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a1942",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def to_lower(text):\n",
    "    return text.str.lower()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    pattern = \"[\" + string.punctuation + \"]+\"\n",
    "    text = text.str.replace(pattern, \" \", regex=True)\n",
    "    text = text.str.replace(\"\\n\", \" \", regex=False)\n",
    "    return text\n",
    "\n",
    "def lemmatize_stopwords(s):\n",
    "    # I combine lemmatization and stopword removal to\n",
    "    # have them both use nlp()\n",
    "    # This is the slow function! Can you do something about it?\n",
    "    doc = nlp(s)\n",
    "    lemma = [] \n",
    "    for token in doc:\n",
    "        if token.text not in stopwords:\n",
    "            lemma.append(token.lemma_)\n",
    "    return ' '.join(lemma)\n",
    "\n",
    "def lsw(text):\n",
    "    return text.apply(lemmatize_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7654113",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text':data.data, 'target':data.target})\n",
    "\n",
    "processed = (df.text.pipe(to_lower)\n",
    "                    .pipe(remove_punctuation)\n",
    "                    .pipe(lsw)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d91bf1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vectorize the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc2c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=4,\n",
    "                             max_df=.5,\n",
    "                             ngram_range=[1,2])\n",
    "bow = vectorizer.fit_transform(processed)\n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf76ed4",
   "metadata": {},
   "source": [
    "## We have a feature matrix for a prediction model!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05924bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Predictive modeling\n",
    "\n",
    "\n",
    "Let's do prediction. This will be a lot of code again, but don't you worry...\n",
    "\n",
    "1. Import the things we need\n",
    "2. Split into a train and test set, so we can honestly assess the predictive power\n",
    "3. Train a Naive Bayes model on the training data\n",
    "4. Inspect the confusion matrix to see how we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71db838",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train-test-split\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, data.target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b551a58e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Instantiate and train model\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Predict test data and assess!\n",
    "y_pred = NB_model.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16df28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# I use a logarithmic color scheme, so the few mismatches are visible!\n",
    "plt.imshow(np.log(cm+1), interpolation='nearest', cmap=plt.cm.Blues)\n",
    "tick_marks = np.arange(len(data.target_names))\n",
    "plt.xticks(tick_marks, data.target_names, rotation=45, ha=\"right\")\n",
    "plt.yticks(tick_marks, data.target_names)\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "cb = plt.colorbar()\n",
    "cb.set_ticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4fd92c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bags-of-Words are unaware of the context and meaning of words\n",
    "\n",
    "Back to SpaCy, because they don't have to be!\n",
    "\n",
    "Word vectors embed a word, based on their \"surroundings\" into a high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0242d5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word vectors from very simple language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df28f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mango = nlp('mango')\n",
    "mango.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc6d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mango = nlp('mango').vector\n",
    "strawberry = nlp('strawberry').vector\n",
    "brick = nlp('brick').vector\n",
    "\n",
    "print(((mango - strawberry)**2).sum())\n",
    "print(((mango - brick)**2).sum())\n",
    "print(((strawberry - brick)**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc3aea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Similarity is also baked in, but only for \"decent\" language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b27307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you do not download the larger language model like below,\n",
    "# spaCy will complain about not having word vectors for the similarity\n",
    "# measures. Download the larger model to your computer using \n",
    "# $ python -m spacy download en_core_web_lg\n",
    "# Note that this takes about 800 MB of disk space\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokens = nlp('mango strawberry brick')\n",
    "for i, token1 in enumerate(tokens):\n",
    "    # similarities are symmetric!\n",
    "    for token2 in tokens[i+1:]:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e5de7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# On to Transformer-based language models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615e2c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
